{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import MaxPooling2D, Flatten, Conv2D\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from matplotlib import pyplot as plt\n",
    "from slackclient import SlackClient\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from space_utils import *\n",
    "\n",
    "from keras import regularizers\n",
    "from time import process_time\n",
    "from shutil import copyfile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import os\n",
    "\n",
    "pd.options.display.max_columns = 45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "---\n",
    "Hi and hello! Welcome to the step-by-step guide of how to train a model to detect supernova.\n",
    "\n",
    "Throughout this guide you will learn about the data that we used, the building of a model in Keras, and how we went about record keeping for our experiments.\n",
    "\n",
    "There is a seperate file called utils.py that holds any functions that we wrote for our project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants\n",
    "---\n",
    "We find it best to define a set of constants at the beginning of the notebook for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_PATH = \"/home/ubuntu\"\n",
    "DATA_PATH = \"/home/ubuntu/data/\"\n",
    "MODEL_PATH = \"/home/ubuntu/model/\"\n",
    "RESULTS_PATH = \"/home/ubuntu/results/\"\n",
    "\n",
    "ALL_DATA_FILE = \"extra_small_all_object_data_in_dictionary_format.pkl\"\n",
    "NORMALIZED_IMAGE_DATA_FILE = \"extra_small_normalized_image_object_data_in_numpy_format.pkl\"\n",
    "\n",
    "MODEL_LOGGING_FILE = \"model_results.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "---\n",
    "We first have to load in the data to be used for model training.\n",
    "\n",
    "This consists of 2 main data files stored in the variables ALL_DATA_FILE and NORMALIZED_IMAGE_DATA_FILE.\n",
    "\n",
    "<strong>ALL_DATA_FILE</strong>: We have any information that will be relevent to an object observation in here. This is a dictionary\n",
    "with 4 keys -- images, targets, file_paths, observation_numbers -- where each key holds a Numpy array. The indices of\n",
    "each array are all properly aligned according to their respective objects (explained in the table).\n",
    "\n",
    "| X_normalized        | X        | Y        | file_path        | observation_number        |\n",
    "|---------------------|----------|----------|------------------|---------------------------|\n",
    "| obj_0_X_normalized  | obj_0_X  | obj_0_Y  | obj_0_file_path  | obj_0_observation_number  |\n",
    "| obj_42_X_normalized | obj_42_X | obj_42_Y | obj_42_file_path | obj_42_observation_number |\n",
    "\n",
    "<strong>NORMALIZED_IMAGE_DATA_FILE</strong>: This is simply a Numpy array of photos ready to be fed into a model. They are normalized and the channels -- search image, template image, difference image -- are organized properly. The preparation of this data is in >>>FILL IN<<<."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pickle.load(open(DATA_PATH + ALL_DATA_FILE, \"rb\"))\n",
    "all_images_normalized = pickle.load(open(DATA_PATH + NORMALIZED_IMAGE_DATA_FILE, \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "---\n",
    "We have to split the data into 3 different sets: training, validation, and testing. Utilizing the *split_space_data*\n",
    "function we imported from *utils.py* this is pretty straightforward.\n",
    "\n",
    "P.S. Sorry that each line is so long... We tried multiple ways of making this easier on the eyes but this makes\n",
    "the most sense!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_train_normal, Y_train, file_path_train, observation_number_train), (X_test, X_test_normal, Y_test, file_path_test, observation_number_test) = split_space_data(\n",
    "    all_images_normalized, \n",
    "    all_data[\"images\"],\n",
    "    all_data[\"targets\"], \n",
    "    all_data[\"file_paths\"], \n",
    "    all_data[\"observation_numbers\"], \n",
    "    0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, X_train_normal, Y_train, file_path_train, observation_number_train), (X_valid, X_valid_normal, Y_valid, file_path_valid, observation_number_valid) = split_space_data(\n",
    "    X_train,\n",
    "    X_train_normal,\n",
    "    Y_train,\n",
    "    file_path_train,\n",
    "    observation_number_train,\n",
    "    0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "---\n",
    "We define the model in a function just to keep things separated nicely. <strong>Feel free to change the model however\n",
    "    you like! Try things out :D </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(X, Y, params):\n",
    "    \n",
    "    # Figure out the data shape\n",
    "    input_shape = (X.shape[1], X.shape[2], X.shape[3])\n",
    "    \n",
    "    # Define the model object to append layers to\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add first layer\n",
    "    model.add(Conv2D(\n",
    "        filters=params[\"NUMBER_OF_FILTERS_1\"],\n",
    "        kernel_size=(3,3),\n",
    "        strides=(1,1),\n",
    "        border_mode='same',\n",
    "        data_format='channels_first',\n",
    "        input_shape=input_shape\n",
    "    ))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(\n",
    "        filters=params[\"NUMBER_OF_FILTERS_1\"],\n",
    "        kernel_size=(3,3),\n",
    "        strides=(2,2),\n",
    "        border_mode='same',\n",
    "        data_format='channels_first',\n",
    "        input_shape=input_shape\n",
    "    ))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # Second layer\n",
    "    model.add(Conv2D(\n",
    "        filters=params[\"NUMBER_OF_FILTERS_2\"],\n",
    "        strides=(1,1),\n",
    "        kernel_size=(3,3),\n",
    "        border_mode='same',\n",
    "        data_format='channels_first',\n",
    "    ))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(\n",
    "        filters=params[\"NUMBER_OF_FILTERS_2\"],\n",
    "        strides=(2,2),\n",
    "        kernel_size=(3,3),\n",
    "        border_mode='same',\n",
    "        data_format='channels_first',\n",
    "    ))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # Third layer\n",
    "    model.add(Conv2D(\n",
    "        filters=params[\"NUMBER_OF_FILTERS_3\"],\n",
    "        strides=(1,1),\n",
    "        kernel_size=(3,3),\n",
    "        border_mode='same',\n",
    "        data_format='channels_first',\n",
    "    ))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(\n",
    "        filters=params[\"NUMBER_OF_FILTERS_3\"],\n",
    "        strides=(2,2),\n",
    "        kernel_size=(3,3),\n",
    "        border_mode='same',\n",
    "        data_format='channels_first',\n",
    "    ))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # Fourth layer\n",
    "    model.add(Conv2D(\n",
    "        filters=params[\"NUMBER_OF_FILTERS_4\"],\n",
    "        strides=(1,1),\n",
    "        kernel_size=(3,3),\n",
    "        border_mode='same',\n",
    "        data_format='channels_first',\n",
    "    ))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(\n",
    "        filters=params[\"NUMBER_OF_FILTERS_4\"],\n",
    "        strides=(2,2),\n",
    "        kernel_size=(3,3),\n",
    "        border_mode='same',\n",
    "        data_format='channels_first',\n",
    "    ))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # Fifth layer\n",
    "    model.add(Conv2D(\n",
    "        filters=params[\"NUMBER_OF_FILTERS_4\"],\n",
    "        strides=(1,1),\n",
    "        kernel_size=(3,3),\n",
    "        border_mode='same',\n",
    "        data_format='channels_first',\n",
    "    ))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # Output layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dropout(params[\"DROPOUT_PERCENT\"]))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters\n",
    "---\n",
    "We have separated parameters into 2 buckets with the folowing definitions:\n",
    "- user_params: Information *about* the model for record keeping\n",
    "- model_params: Information *for* the model to consume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_params = {\n",
    "    \"INITIALS\": \"cc\",\n",
    "    \"MODEL_DESCRIPTION\": \"My first public model!\",\n",
    "    \"VERSION\": \"1\"\n",
    "}\n",
    "\n",
    "model_params = {\n",
    "    \"LEARNING_RATE\": 0.00014148226882681195,\n",
    "    \"BATCH_SIZE\": 368,\n",
    "    \"DROPOUT_PERCENT\": 0.4488113054975806,\n",
    "    \"NUMBER_OF_FILTERS_1\": 25,\n",
    "    \"NUMBER_OF_FILTERS_2\": 63,\n",
    "    \"NUMBER_OF_FILTERS_3\": 119,\n",
    "    \"NUMBER_OF_FILTERS_4\": 210,    \n",
    "    \"NUMBER_OF_EPOCHS\": 40,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Experimentation\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_AMOUNT = 1\n",
    "\n",
    "for current_model_number in range(MODEL_AMOUNT):\n",
    "    \n",
    "    # Indicate and log model start\n",
    "    print(\"START MODEL SEARCH (model {} of {})\".format(current_model_number, MODEL_AMOUNT))\n",
    "    start = process_time()\n",
    "    \n",
    "    # Randomize specific parameters if we are doing a search\n",
    "    # Feel free to add or change the current parameters\n",
    "    if MODEL_AMOUNT > 1:\n",
    "        params[\"LEARNING_RATE\"] = 10 ** np.random.uniform(-4, -2)\n",
    "        params[\"BATCH_SIZE\"] = 16 * np.random.randint(1, 96)\n",
    "        params[\"DROPOUT_PERCENT\"] = np.random.uniform(0.0, 0.6)\n",
    "        params[\"NUMBER_OF_FILTERS_1\"] = np.random.randint(4, 32)\n",
    "        params[\"NUMBER_OF_FILTERS_2\"] = np.random.randint(16, 64)\n",
    "        params[\"NUMBER_OF_FILTERS_3\"] = np.random.randint(32, 128)\n",
    "        params[\"NUMBER_OF_FILTERS_4\"] = np.random.randint(64, 256) \n",
    "        \n",
    "    # Build the model and catch if the model acrhitectur is not valid\n",
    "    try:\n",
    "        model = build_model(X_train, Y_train, model_params)\n",
    "    except Exception as e:\n",
    "        print(\"That didn't work!\")\n",
    "        print(e)\n",
    "        continue\n",
    "        \n",
    "    # Create the specific model name\n",
    "    model_name = user_params[\"INITIALS\"] + \"_convolutional_\" + str(user_params[\"VERSION\"]) + str(current_model_number)\n",
    "    user_params[\"VERSION\"] = user_params[\"VERSION\"] + str(1)\n",
    "    \n",
    "    # Define an optimizer for the model\n",
    "    adam_optimizer = Adam(\n",
    "        lr=model_params[\"LEARNING_RATE\"], \n",
    "        beta_1=0.9, \n",
    "        beta_2=0.999, \n",
    "        epsilon=None, \n",
    "        decay=0.0\n",
    "    )\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\", \n",
    "        optimizer=adam_optimizer,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Figure out where to save the model checkpoints\n",
    "    checkpoint_file = MODEL_PATH + \"mdl.hdf5\"\n",
    "    checkpointer = ModelCheckpoint(filepath=checkpoint_file, verbose=2, save_best_only=True)\n",
    "    \n",
    "    # Create an early stopping callback\n",
    "    early_stopping_callback = EarlyStopping(patience=5, min_delta=0.0005, verbose=2)\n",
    "    \n",
    "    # Actually train the model\n",
    "    print(model_params)\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        Y_train,\n",
    "        batch_size=model_params[\"BATCH_SIZE\"],\n",
    "        nb_epoch=model_params[\"NUMBER_OF_EPOCHS\"],\n",
    "        verbose=1,\n",
    "        validation_data=(X_valid, Y_valid),\n",
    "        callbacks=[checkpointer, early_stopping_callback]\n",
    "    )\n",
    "    \n",
    "    # Reload the best model\n",
    "    model = load_model(checkpoint_file)\n",
    "    \n",
    "    # Get final predictions for the model and write to a file\n",
    "    predictions = model.predict(X_test).flatten()\n",
    "    model_metrics = get_metrics(predictions, Y_test)\n",
    "    create_result_csv(user_params, model_params, model_metrics, file_name=RESULTS_PATH + MODEL_LOGGING_FILE)\n",
    "    \n",
    "    # Save the model to a unique location if the Pippin metric is better than the papers\n",
    "    if model_metrics[\"PIPPIN_METRIC\"] < 0.202:\n",
    "        copyfile(checkpoint_file, checkpoint_filepath + \"{}.hdf5\".format(model_name))\n",
    "        \n",
    "    # Plot the model history\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Training History')\n",
    "    plt.ylabel('Binary Cross Entropy Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.xlim([0, len(history.history['loss'])])\n",
    "    plt.legend(['Training set', 'Validation set'], loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "    # Reset plot to clean up extra lines\n",
    "    plt.clf()\n",
    "    \n",
    "    # Get some indication of process length\n",
    "    final = process_time()\n",
    "    print('FINISHED MODEL SEARCH. {} SECONDS.'.format(str(final-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
